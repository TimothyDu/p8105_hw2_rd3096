---
title: "p8105_hw2_rd3096"
output: github_document
---

```{r setup}
library(tidyverse)
```

# problem1 

## load the nyc transit data and clean names
```{r}
nyc_subway=
  read_csv("./NYCSUB.csv",na=c("NA","",".")) %>% 
  janitor::clean_names()
```

## clean the data as requested 
```{r}
nyc_subway_clean=nyc_subway %>% 
  select(
    line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(
    entry=
      case_match(
      entry,
      "YES"~TRUE,
      "NO"~ FALSE),
      entry=as.logical(entry)
    )
```

## a short paragraph describing the dataset
The NYC subway dataset contains 1,868 rows and 19 columns. It includes variables such as line, which represents the subway line (e.g., "4 Avenue"), station_name for the subway station (e.g., "25th St"), geographical coordinates (station_latitude and station_longitude), and various subway routes from route1 to route11. Additionally, the dataset has information on the type of entrance (entrance_type), whether vending machines are available (vending), and whether the station is ADA compliant (ada).
For the cleaning process, I retained only the necessary variables and transformed the entry variable, which originally contained "YES" and "NO" as character values, into a logical format (TRUE/FALSE). After cleaning, the dataset consists of 1,868 observations and retains the 19 original columns. While the data is fairly tidy, further steps may be needed to handle the route variables, which are currently spread across multiple columns.

## Answer the Questions:
*How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

```{r}
distinct_stations=
  nyc_subway_clean %>% 
  distinct(station_name,line) %>% 
  nrow()
```
1. There are 465 number of distinct stations are stored in distinct_stations.

*How many stations are ADA compliant?
```{r}
ada_compliant_stations=
  nyc_subway_clean %>% 
  filter(ada==TRUE) %>% 
  nrow()
```
2. There are 468 stations are ADA compliant.

*What proportion of station entrances / exits without vending allow entrance?
```{r}
proportion_no_vending_allows_entry=
  nyc_subway_clean %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry))
```
3. 37.7% of station entrances / exits without vending allow entrance.

## reformat the data to make route number and rount names distinct variables
```{r}
nyc_subway_routes_clean= 
  nyc_subway_clean %>%
  mutate(across(route1:route11,as.character)) %>% 
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name" 
  )
```

## number of distinct stations serve the A train
```{r}
a_train_stations=
  nyc_subway_routes_clean %>%
  filter(route_name == "A") %>% 
  distinct(station_name, line) %>% 
  nrow()
```
There are 60 disctinct stations serve the A train.

## number of distinct stations serve the A train that are complaint train
```{r}
# Find ADA-compliant stations that serve the A train
ada_compliant_a_train_stations=
  nyc_subway_routes_clean %>%
  filter(route_name == "A", ada == TRUE) %>%
  distinct(station_name, line) %>% 
  nrow()
```
Of the stations that serve the A train, 17 are ADA compliant

# problem 2
```{r}
library(readxl)
library(dplyr)
```

## read the Mr. Trash Wheel excel file and clean the data
```{r}
mr_trash_wheel=
  read_excel("./TrashWheel.xlsx", sheet="Mr. Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls=as.integer(round(sports_balls))) %>%
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Mr. Trash Wheel")
```
## read the Professor Trash Wheel excel file and clean the data
```{r}
professor_trash_wheel=
  read_excel("./trashwheel.xlsx", sheet="Professor Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Professor. Trash Wheel")
```
## read the Professor Trash Wheel excel file and clean the data
```{r}
gwynnda_trash_wheel=
  read_excel("./trashwheel.xlsx", sheet="Gwynnda Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Gwynnda. Trash Wheel")
```

## combine the datafiles using binding
```{r}
combined_trash_wheel=
  bind_rows(
    mr_trash_wheel, 
    professor_trash_wheel, 
    gwynnda_trash_wheel) %>% 
  relocate(trash_wheel)
```

The number of observations in the combined datafile is `r nrow(combined_trash_wheel)`.Key variables in the dataset include `dumpster`, which identifies the dumpster used, `weight_tons` for the weight of trash collected, and `cigarette_butts` for the number of cigarette butts collected.

```{r}
total_weight_professor= combined_trash_wheel %>%
  filter(trash_wheel == "Professor. Trash Wheel") %>%
  summarize(total_weight = sum(weight_tons, na.rm = TRUE)) %>% 
pull(total_weight)


total_cig_butts_gwynnda_june=
  combined_trash_wheel %>%
  filter(trash_wheel == "Gwynnda. Trash Wheel", month == "June", year == "2022") %>%
  summarize(total_cig_butts = sum(cigarette_butts, na.rm = TRUE)) %>% 
  pull(total_cig_butts)
```
The total weight of trash collected by Professor Trash Wheel is `r total_weight_professor`, the total number of cigarette butts collected by Gwynnda in June of 2022 is `r total_cig_butts_gwynnda_june`.


# Problem 3

## import, clean and Tidy three datafiles, bakers, bakes and results.

```{r}
baker_clean=
  read_csv("./gbb/bakers.csv",na=c("NA","",".")) %>% 
  janitor::clean_names() %>% 
  separate(baker_name, into=c("baker","last_name"),sep = " ", extra = "merge") %>% 
  relocate(baker) %>% 
  arrange(baker)

bake_clean=
  read_csv("./gbb/bakes.csv", na=c("N/A","","UNKNOWN","Unknown")) %>%
  janitor::clean_names() %>% 
  relocate(baker)

result_clean=
  read_csv("./gbb/results.csv",na=c("NA","","."),skip=2) %>% 
  janitor::clean_names() %>% 
  relocate(baker)

head(baker_clean)
head(bake_clean)
head(result_clean)
```

## check for check for completeness and correctness across datasets 
```{r}
# Identify bakers without corresponding bakes
bakers_missing_bakes =
  anti_join(baker_clean, bake_clean, by = "series", "baker")
str(bakers_missing_bakes)

bakers_missing_result=
  anti_join(baker_clean, result_clean, by = c ("series","baker"))
str(bakers_missing_result)
```
there are 25 rows in the baker_clean that doesnot exist in bake_clean
there is one record in baker_clean that deosnot exist in result_clean

## Merge the datasets to create a single final dataset
```{r}
baker_bake=
  left_join(baker_clean, bake_clean,by=c("baker","series")) 

bake_combine=
  left_join(baker_bake,result_clean,by=c("baker","episode","series")) %>% 
  janitor::clean_names() %>% 
  relocate(baker,series,episode)
```

## exporting datafile bake_combine

```{r}
write_csv(bake_combine, "./gbb/bake_combine.csv")
```

## My data cleaning process:

1. Step 1: Load and Inspect the Data

2. Step 2: Handle Missing Values and Inconsistent Data

3. step 3: Split the Baker's Name into First and Last Name

5. Step 4: Merge and export the Datasets

Discussion of the final dataset bake_combined:
The final dataset has `r nrow(bake_combine)` rows and `r ncol(bake_combine)`columns, it contains information about each baker, their bakes, and their performance results. Key variables include:

*Baker Information: baker, last_name, series, and baker_age.

*Bake Information: signature_bake, technical_bake, and showstopper_bake.
Performance Results: technical_rank, result (whether the baker stayed, was eliminated, or became Star Baker).

The dataset is ordered by series and episode, making it easy to track the performance for each baker across different challenges throughout the show.

## create a table shows showing the star baker or winner of each episode in Seasons 5 through 10
```{r}
star_win=
  bake_combine %>% 
  filter(series >= 5 & series <= 10,result %in% c("STAR BAKER","WINNER")) %>%
  select(baker,series,episode,result) %>% 
  arrange(series,episode) %>% 
  knitr::kable()
star_win
```
In summary, Candice’s dominance in Season 7, with multiple Star Baker titles and ultimately winning the competition, stands out as predictable. Ian's inability to carry his early momentum to an overall win in Season 6 could be seen as a surprise. Meanwhile, other contestants, like Andrew and Benjamin, performed well but couldn’t match Candice’s level of baking.

## import clean and tidy the viewer dataset
```{r}
viewer_clean=
  read_csv("./gbb/viewers.csv",na=c("NA","",".")) %>% 
  janitor::clean_names() 

head(viewer_clean,10)
```
The average viewership in Season 1 is `r mean(pull(viewer_clean, series_1), na.rm = TRUE)`.
The average viewership in Season 5 is `r mean(pull(viewer_clean, series_5), na.rm = TRUE)`.


