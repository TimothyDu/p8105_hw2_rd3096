---
title: "p8105_hw2_rd3096"
output: github_document
---

```{r setup}
library(tidyverse)
```

#problem1 

##load the nyc transit data and clean names
```{r}
nyc_subway=
  read_csv("./NYCSUB.csv",na=c("NA","",".")) %>% 
  janitor::clean_names()
```

##clean the data as requested 
```{r}
nyc_subway_clean=nyc_subway %>% 
  select(
    line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(
    entry=
      case_match(
      entry,
      "YES"~TRUE,
      "NO"~ FALSE),
      entry=as.logical(entry)
    )
```

## a short paragraph describing the dataset
The NYC subway dataset contains 1,868 rows and 19 columns. It includes variables such as line, which represents the subway line (e.g., "4 Avenue"), station_name for the subway station (e.g., "25th St"), geographical coordinates (station_latitude and station_longitude), and various subway routes from route1 to route11. Additionally, the dataset has information on the type of entrance (entrance_type), whether vending machines are available (vending), and whether the station is ADA compliant (ada).
For the cleaning process, I retained only the necessary variables and transformed the entry variable, which originally contained "YES" and "NO" as character values, into a logical format (TRUE/FALSE). After cleaning, the dataset consists of 1,868 observations and retains the 19 original columns. While the data is fairly tidy, further steps may be needed to handle the route variables, which are currently spread across multiple columns.

##Answer the Questions:
*How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

```{r}
distinct_stations=
  nyc_subway_clean %>% 
  distinct(station_name,line) %>% 
  nrow()
```
1. There are 465 number of distinct stations are stored in distinct_stations.

*How many stations are ADA compliant?
```{r}
ada_compliant_stations=
  nyc_subway_clean %>% 
  filter(ada==TRUE) %>% 
  nrow()
```
2. There are 468 stations are ADA compliant.

*What proportion of station entrances / exits without vending allow entrance?
```{r}
proportion_no_vending_allows_entry=
  nyc_subway_clean %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry))
```
3. 37.7% of station entrances / exits without vending allow entrance.

##reformat the data to make route number and rount names distinct variables
```{r}
nyc_subway_routes_clean= 
  nyc_subway_clean %>%
  mutate(across(route1:route11,as.character)) %>% 
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name" 
  )
```

##number of distinct stations serve the A train
```{r}
a_train_stations=
  nyc_subway_routes_clean %>%
  filter(route_name == "A") %>% 
  distinct(station_name, line) %>% 
  nrow()
```
There are 60 disctinct stations serve the A train.

##number of distinct stations serve the A train that are complaint train
```{r}
# Find ADA-compliant stations that serve the A train
ada_compliant_a_train_stations=
  nyc_subway_routes_clean %>%
  filter(route_name == "A", ada == TRUE) %>%
  distinct(station_name, line) %>% 
  nrow()
```
Of the stations that serve the A train, 17 are ADA compliant

#problem 2
```{r}
library(readxl)
library(dplyr)
```

##read the Mr. Trash Wheel excel file and clean the data
```{r}
mr_trash_wheel=
  read_excel("./trashwheel.xlsx", sheet="Mr. Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls=as.integer(round(sports_balls))) %>%
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Mr. Trash Wheel")
```
##read the Professor Trash Wheel excel file and clean the data
```{r}
professor_trash_wheel=
  read_excel("./trashwheel.xlsx", sheet="Professor Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Professor. Trash Wheel")
```
##read the Professor Trash Wheel excel file and clean the data
```{r}
gwynnda_trash_wheel=
  read_excel("./trashwheel.xlsx", sheet="Gwynnda Trash Wheel", skip=1) %>% janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  select(dumpster:homes_powered) %>% 
  mutate(year = as.character(year)) %>% 
mutate(trash_wheel = "Gwynnda. Trash Wheel")
```

## combine the datafiles using binding
```{r}
combined_trash_wheel=
  bind_rows(
    mr_trash_wheel, 
    professor_trash_wheel, 
    gwynnda_trash_wheel) %>% 
  relocate(trash_wheel)
```

The number of observations in the combined datafile is `r nrow(combined_trash_wheel)`.Key variables in the dataset include `dumpster`, which identifies the dumpster used, `weight_tons` for the weight of trash collected, and `cigarette_butts` for the number of cigarette butts collected.

```{r}
total_weight_professor= combined_trash_wheel %>%
  filter(trash_wheel == "Professor. Trash Wheel") %>%
  summarize(total_weight = sum(weight_tons, na.rm = TRUE)) %>%
  pull(total_weight)

total_cig_butts_gwynnda_june=
  combined_trash_wheel %>%
  filter(trash_wheel == "Gwynnda. Trash Wheel", month == "June", year == "2022") %>%
  summarize(total_cig_butts = sum(cigarette_butts, na.rm = TRUE)) %>%
  pull(total_cig_butts)
```
The total weight of trash collected by Professor Trash Wheel is 216.26, the total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.


#Problem 3

import, clean and Tidy baker.csv

```{r}
baker_clean=
  read_csv("./gbb/bakers.csv",na=c("NA","",".")) %>% 
  janitor::clean_names() %>% 
  separate(baker_name, into=c("baker","last_name"),sep = " ", extra = "merge") %>% 
  relocate(baker) %>% 
  arrange(baker)

bake_clean=
  read_csv("./gbb/bakes.csv", na=c("N/A","","UNKNOWN","Unknown")) %>%
  janitor::clean_names() %>% 
  relocate(baker)

result_clean=
  read_csv("./gbb/results.csv",na=c("NA","","."),skip=2) %>% 
  janitor::clean_names() %>% 
  relocate(baker)

head(baker_clean)
head(bake_clean)
head(result_clean)
```

check for check for completeness and correctness across datasets 
```{r}
# Identify bakers without corresponding bakes
bakers_missing_bakes =
  anti_join(baker_clean, bake_clean, by = "baker") %>% 
  anti_join(baker_clean, result_clean, by = "baker")
head(bakers_missing_bakes)
```
there are 0 no-corresponding values

Merge the datasets to create a single final dataset
```{r}
baker_bake=
  left_join(baker_clean, bake_clean,by=c("baker","series")) %>% 
  arrange(episode)

result_clean=result_clean %>% arrange(episode)

bake_combine=
  left_join(baker_bake,result_clean,by=c("baker","episode","series")) 
```

exporting datafile bake_combine

```{r}
write_csv(bake_combine, "./gbb/bake_combine.csv")
```

My data cleaning process:
1. Step 1: Load and Inspect the Data

2. Step 2: Handle Missing Values and Inconsistent Data

3. Step 3: Identify Many-to-Many Relationships:some bakers had multiple
entries in both the bake_clean and result_clean datasets.

4. step 4: Split the Baker's Name into First and Last Name

5. Step 5: Merge and export the Datasets

Discussion of the final dataset bake_combined:
The final dataset is a merged dataset that contains information about each baker, their bakes, and their performance results. Key variables include:
*Baker Information: baker_name, last_name, series, and baker_age.

*Bake Information: signature_bake, technical_bake, and showstopper_bake.
Performance Results: technical_rank, result (whether the baker stayed, was eliminated, or became Star Baker).

The dataset is ordered by series and episode, making it easy to track the performance for each baker across different challenges throughout the show.

##create a table shows showing the star baker or winner of each episode in Seasons 5 through 10
```{r}

```

